{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading and fetching the housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now load the data using Pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a Look at the Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = load_housing_data(housing_path=HOUSING_PATH)\n",
    "housing.head(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row represents one district. There are 10 attributes: longitude, latitude, housing_median_age, total_rooms, total_bed, rooms, population, households, median_income, median_house_value, and ocean_proximity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The info() method is useful to get a quick description of the data the total number of rows, and each attribute’s type and number of non-null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All attributes are numerical, except the ocean_proximity field. \n",
    "* It is an object, so it could hold any kind of Python object.\n",
    "* It is a categorical attribute. \n",
    "* You can find out what categories exist and how many districts belong to each category by using the value_counts() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The describe() method shows a summary of the numerical attributes\n",
    "housing[\"ocean_proximity\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# he describe() method shows a summary of the numerical attributes\n",
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to know the data is to plot a histogram for each numerical attribute.  A histogram shows the number of instances (on the vertical axis) that have a given value range (on the horizontal axis). \n",
    "- You can either plot this one attribute at a time, or \n",
    "- you can call the hist() method on the whole dataset, and it will plot a histogram for each numerical attribute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = split_train_test(housing, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or instead you can use the Scikit-Learn  provides few functions to split datasetsinto into multiple subsets.\n",
    "\n",
    "The simplest function is train_test_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stratified sampling is better than random sampling\n",
    "* Do stratified sampling based on the income category. \n",
    "* use Scikit-Learn’s StratifiedShuffleSplit class to do that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5])\n",
    "#housing[\"income_cat\"].describe()\n",
    "housing[\"income_cat\"].hist()\n",
    "housing[\"income_cat\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at the income category proportions in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].value_counts() / len(housing[\"income_cat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set[\"income_cat\"].value_counts() / len(strat_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the income_cat attribute so the data is back to its original state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def income_cat_proportions(data):\n",
    "    return data[\"income_cat\"].value_counts() / len(data)\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall\": income_cat_proportions(housing),\n",
    "    \"Stratified\": income_cat_proportions(strat_test_set),\n",
    "    \"Random\": income_cat_proportions(test_set),\n",
    "}).sort_index()\n",
    "compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
    "compare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set, strat_test_set): \n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Geographical Data\n",
    "Since there is geographical information (latitude and longitude), it is a good idea to\n",
    "create a scatterplot of all districts to visualize the data \n",
    "\n",
    "* use alpha = 0.1 to better visulize where is a population high density\n",
    "* make a copy of the training data set the actual training doesn't get changed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=.1)\n",
    "#strat_train_set.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the housing prices. \n",
    "* The radius of each circle represents the district’s population (option s)\n",
    "* The color represents the price (option c). \n",
    "* We will use a predefined color map (option cmap) called jet, which ranges from blue (low values) to red (high prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
    "c=housing['total_rooms']/100, cmap=plt.get_cmap(\"jet\"), colorbar=True, )\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load california maps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the California image\n",
    "images_path = os.path.join(PROJECT_ROOT_DIR, \"images\", \"end_to_end_project\")\n",
    "os.makedirs(images_path, exist_ok=True)\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "filename = \"california.png\"\n",
    "print(\"Downloading\", filename)\n",
    "url = DOWNLOAD_ROOT + \"images/end_to_end_project/\" + filename\n",
    "urllib.request.urlretrieve(url, os.path.join(images_path, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "california_img=mpimg.imread(os.path.join(images_path, filename))\n",
    "ax = housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,7),\n",
    "                       s=housing['population']/100, label=\"Population\",\n",
    "                       c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\n",
    "                       colorbar=False, alpha=0.4,\n",
    "                      )\n",
    "plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5,\n",
    "           cmap=plt.get_cmap(\"jet\"))\n",
    "plt.ylabel(\"Latitude\", fontsize=14)\n",
    "plt.xlabel(\"Longitude\", fontsize=14)\n",
    "\n",
    "prices = housing[\"median_house_value\"]\n",
    "tick_values = np.linspace(prices.min(), prices.max(), 11)\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_yticklabels([\"$%dk\"%(round(v/1000)) for v in tick_values], fontsize=14)\n",
    "cbar.set_label('Median House Value', fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking for Correlations\n",
    "You can easily compute the standard correlation coefficient (also called Pearson’s r) between every pair of attributes using the corr() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to check for correlation between attributes is to use Pandas’ scatter_matrix function, which plots every numerical attribute against every other numerical attribute.\n",
    "\n",
    "* so let’s just focus on a few promising attributes that seem most correlated with the median housing value\n",
    "\n",
    "* The main diagonal (top left to bottom right) would be full of straight lines if Pandas\n",
    "plotted each variable against itself, which would not be very useful. \n",
    "\n",
    "* Pandas displays a histogram of each attribute in the main diagonal instead of the corr of the attribute with itself which is one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median income is the most correlated attribute with the median house value.\n",
    "\n",
    "* let’s zoom in on their correlation scatterplot\n",
    "\n",
    "* the plot reveals horizontal lines around $450,000, $350,000, $280,000, and a few more below that.\n",
    "* You may want to try removing the corresponding districts to prevent the algorithms from learning to reproduce these data quirks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Attribute Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Look at the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The bedrooms_per_room is more correlated with the median house value than the total number of rooms or bedrooms. \n",
    "- houses with a lower bedroom/room ratio tend to be more expensive. \n",
    "- The number of rooms per household is more informative than the total number of rooms in a district—obviously --> the larger the houses, the more expensive they are.\n",
    "\n",
    "- Repeat this process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"rooms_per_household\", y=\"median_house_value\",\n",
    "             alpha=0.2)\n",
    "plt.axis([0, 5, 0, 520000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Data for Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "len(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "- Most Machine Learning algorithms cannot work with missing features\n",
    "- The total_bedrooms attribute has some missing values. We have three options:\n",
    "<ol>\n",
    "<li>  Get rid of the corresponding districts.</li>\n",
    "<li>  Get rid of the whole attribute.</li>\n",
    "<li>  Set the values to some value (zero, the mean, the median, etc.).</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can do this using pandas DataFrame’s dropna(), drop(), and fillna()\n",
    "\n",
    "- dropna() : drop all rows with NA (not available - NaN, pandas.NaT, None)\n",
    "- drop() : Remove rows or columns by specifying label names and corresponding axis.\n",
    "- fillna() : Fill NA/NaN values using the specified method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.dropna(subset=[\"total_bedrooms\"]) # option 1\n",
    "housing.drop(\"total_bedrooms\", axis=1) # option 2\n",
    "median = housing[\"total_bedrooms\"].median() # option 3\n",
    "housing[\"total_bedrooms\"].fillna(median, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn also provides a class to take care of missing values: SimpleImputer.\n",
    "- First, you need to create a SimpleImputer instance, specifying\n",
    "that you want to replace each attribute’s missing values with the median of that attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1) # a numeric copy\n",
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imputer has computed the median of each attribute and stored the result in its statistics_ instance variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.median().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use this “trained” imputer to transform the training set by replacing missing values by the learned medians:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(housing_num)\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a plain NumPy array containing the transformed features. Put it back into a Pandas DataFrame, it’s simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns)\n",
    "housing_tr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Text and Categorical Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ocean_proximity is a categorial (nominal, text) attribute that we cannot compute its median.\n",
    "\n",
    "Many machine learning algorithms require all input and output variables to be numeric (scikit-learn require this)\n",
    "\n",
    "Two approaches to convert these categories (nominal) from text to number:\n",
    "1) ordinal-encoding \n",
    "2) one-hot-encoding (dummy attributes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinal-encoding\n",
    "Ordinal variables: consists of a finite set of discrete values with an ordering between values.\n",
    "\n",
    "Ordinal-encoding: each category value is assigned an integer value. The values have a natural ordered relationship between them.\n",
    "\n",
    "Use Scikit-Learn’s OrdinalEncoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "housing_cat.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can get the list of categories using the categories_ instance variable.\n",
    "ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue: ML algorithms will assume that two nearby values are more similar than two distant values. It imposes an ordinal relationship where no such relationship may exist.\n",
    "\n",
    "* In some cases this works: (e.g., for ordered categories such as “bad”, “average”, “good”, “excellent”)\n",
    "* but it doesn't make sense in other cases: e.g. ocean_proximity column. \n",
    "\n",
    "Solution: one-hot-encoding \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot-encoding  (dummy attributes)\n",
    "\n",
    "- One binary attribute per category: one attribute equal to 1 when the category is “<1H OCEAN” (and 0 otherwise)\n",
    "- Another attribute equal to 1 when the category is “INLAND” (and 0 otherwise), and so on\n",
    "\n",
    "Scikit-Learn provides a OneHotEncoder class to convert categorical values into one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot.toarray()   # convert from scipy sparse matrix object to numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the catagories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Transformers\n",
    "\n",
    "* To write your own transformers for cleanup operations or combining specific attributes. \n",
    "* You need to create a class and implement three methods: \n",
    "**  fit() (returning self)\n",
    "**  transform()\n",
    "** fit_transform(): obtained by adding TransformerMixin as a base class \n",
    "* add BaseEstimator as a base class to get the methods get_params() and set_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=True)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_extra_attribs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.min()\n",
    "%%latex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Feature Scaling\n",
    "* Important transformations to apply to data\n",
    "* Many machine Learning algorithms don’t perform well when the input attributes have very different scales. \n",
    "\n",
    "##### For housing data: \n",
    "* The total number of rooms ranges from 6 to 39,320\n",
    "* The median incomes range from 0 to 15 \n",
    "* Scaling the target is generally not required.\n",
    "\n",
    "##### Two ways to get all attributes to have the same scale\n",
    "* min-max scaling (normalization): values are shifted and rescaled so that they end up ranging from 0 to 1. Subtract the min value and divide by the max minus the min. $ x = \\frac {x-min} {max - min} $\n",
    "\n",
    "    * Scikit-Learn provides a transformer MinMaxScaler for this.  \n",
    "\n",
    "* Standardization:  subtracts the mean value, then divides by the standard deviation  resulting distribution has unit variance.\n",
    "$$ y = \\frac {(x – mean)} / stddev $$\n",
    "    * It does not bound values to a specific range. \n",
    "    * may be a problem for some algorithms\n",
    "    * Less affected by outliers\n",
    "    * Scikit-Learn provides a transformer called StandardScaler for standardization.\n",
    "\n",
    "##### With all the transformations, it is important to fit the scalers to the training data only, not to the full dataset. Only then can you use them to transform the training set and the test set (and new data).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a normalization\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "data = np.asarray([[100, 0.001],\n",
    "                [8, 0.05],\n",
    "                [50, 0.005],\n",
    "                [88, 0.07],\n",
    "                [4, 0.1]])\n",
    "print(data)\n",
    "# define min max scaler\n",
    "scaler = MinMaxScaler()\n",
    "# transform data\n",
    "scaled = scaler.fit_transform(data)\n",
    "print(scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "housing.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "housing[['total_rooms']] =hscaler.fit_transform(housing[['total_rooms']])\n",
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "housing[['total_rooms']] =scaler.fit_transform(housing[['total_rooms']])\n",
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation Pipelines\n",
    "* Many data transformations need to be executed in the right order. Sequences of different transforms can be chained together using the Pipeline, such as imputing missing values, then scaling numerical values, categorial encoding.\n",
    "* The Pipeline constructor takes a list of name/estimator pairs defining a sequence of steps. \n",
    "* All but the last estimator must be transformers (i.e., they must have a fit_transform() method). \n",
    "* The names can be anything as they are unique and don’t contain double underscores “__”: they are convenient for hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['longitude',\n",
       " 'latitude',\n",
       " 'housing_median_age',\n",
       " 'total_rooms',\n",
       " 'total_bedrooms',\n",
       " 'population',\n",
       " 'households',\n",
       " 'median_income']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "num_pipeline = Pipeline([ ('imputer', SimpleImputer(strategy=\"median\")), \n",
    "                         ('attribs_adder', CombinedAttributesAdder()), \n",
    "                         ('std_scaler', StandardScaler()),])\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.651200e+04</td>\n",
       "      <td>1.651200e+04</td>\n",
       "      <td>1.651200e+04</td>\n",
       "      <td>1.651200e+04</td>\n",
       "      <td>1.651200e+04</td>\n",
       "      <td>1.651200e+04</td>\n",
       "      <td>1.651200e+04</td>\n",
       "      <td>1.651200e+04</td>\n",
       "      <td>1.651200e+04</td>\n",
       "      <td>1.651200e+04</td>\n",
       "      <td>1.651200e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-4.353107e-15</td>\n",
       "      <td>2.284564e-15</td>\n",
       "      <td>-4.701235e-17</td>\n",
       "      <td>7.587062e-17</td>\n",
       "      <td>1.360615e-16</td>\n",
       "      <td>-3.700743e-17</td>\n",
       "      <td>2.078979e-17</td>\n",
       "      <td>-2.076289e-16</td>\n",
       "      <td>8.014691e-17</td>\n",
       "      <td>-1.621765e-17</td>\n",
       "      <td>-4.878742e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000030e+00</td>\n",
       "      <td>1.000030e+00</td>\n",
       "      <td>1.000030e+00</td>\n",
       "      <td>1.000030e+00</td>\n",
       "      <td>1.000030e+00</td>\n",
       "      <td>1.000030e+00</td>\n",
       "      <td>1.000030e+00</td>\n",
       "      <td>1.000030e+00</td>\n",
       "      <td>1.000030e+00</td>\n",
       "      <td>1.000030e+00</td>\n",
       "      <td>1.000030e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.384937e+00</td>\n",
       "      <td>-1.449760e+00</td>\n",
       "      <td>-2.199168e+00</td>\n",
       "      <td>-1.223689e+00</td>\n",
       "      <td>-1.294944e+00</td>\n",
       "      <td>-1.269921e+00</td>\n",
       "      <td>-1.317668e+00</td>\n",
       "      <td>-1.772116e+00</td>\n",
       "      <td>-1.650273e+00</td>\n",
       "      <td>-2.075303e-01</td>\n",
       "      <td>-2.704542e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.111083e+00</td>\n",
       "      <td>-7.949406e-01</td>\n",
       "      <td>-8.472092e-01</td>\n",
       "      <td>-5.516890e-01</td>\n",
       "      <td>-5.793145e-01</td>\n",
       "      <td>-5.698825e-01</td>\n",
       "      <td>-5.803963e-01</td>\n",
       "      <td>-6.870806e-01</td>\n",
       "      <td>-3.822514e-01</td>\n",
       "      <td>-5.741738e-02</td>\n",
       "      <td>-5.914834e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.324379e-01</td>\n",
       "      <td>-6.452675e-01</td>\n",
       "      <td>2.758786e-02</td>\n",
       "      <td>-2.353301e-01</td>\n",
       "      <td>-2.458409e-01</td>\n",
       "      <td>-2.292746e-01</td>\n",
       "      <td>-2.370459e-01</td>\n",
       "      <td>-1.756999e-01</td>\n",
       "      <td>-7.966522e-02</td>\n",
       "      <td>-2.406537e-02</td>\n",
       "      <td>-1.628895e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.822131e-01</td>\n",
       "      <td>9.730728e-01</td>\n",
       "      <td>6.638039e-01</td>\n",
       "      <td>2.423650e-01</td>\n",
       "      <td>2.604547e-01</td>\n",
       "      <td>2.684162e-01</td>\n",
       "      <td>2.793106e-01</td>\n",
       "      <td>4.561338e-01</td>\n",
       "      <td>2.358755e-01</td>\n",
       "      <td>1.596812e-02</td>\n",
       "      <td>4.044268e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.630550e+00</td>\n",
       "      <td>2.951564e+00</td>\n",
       "      <td>1.856709e+00</td>\n",
       "      <td>1.716114e+01</td>\n",
       "      <td>1.381603e+01</td>\n",
       "      <td>3.071047e+01</td>\n",
       "      <td>1.293803e+01</td>\n",
       "      <td>5.839969e+00</td>\n",
       "      <td>5.225419e+01</td>\n",
       "      <td>1.070603e+02</td>\n",
       "      <td>3.975916e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          longitude      latitude  housing_median_age   total_rooms  \\\n",
       "count  1.651200e+04  1.651200e+04        1.651200e+04  1.651200e+04   \n",
       "mean  -4.353107e-15  2.284564e-15       -4.701235e-17  7.587062e-17   \n",
       "std    1.000030e+00  1.000030e+00        1.000030e+00  1.000030e+00   \n",
       "min   -2.384937e+00 -1.449760e+00       -2.199168e+00 -1.223689e+00   \n",
       "25%   -1.111083e+00 -7.949406e-01       -8.472092e-01 -5.516890e-01   \n",
       "50%    5.324379e-01 -6.452675e-01        2.758786e-02 -2.353301e-01   \n",
       "75%    7.822131e-01  9.730728e-01        6.638039e-01  2.423650e-01   \n",
       "max    2.630550e+00  2.951564e+00        1.856709e+00  1.716114e+01   \n",
       "\n",
       "       total_bedrooms    population    households  median_income  \\\n",
       "count    1.651200e+04  1.651200e+04  1.651200e+04   1.651200e+04   \n",
       "mean     1.360615e-16 -3.700743e-17  2.078979e-17  -2.076289e-16   \n",
       "std      1.000030e+00  1.000030e+00  1.000030e+00   1.000030e+00   \n",
       "min     -1.294944e+00 -1.269921e+00 -1.317668e+00  -1.772116e+00   \n",
       "25%     -5.793145e-01 -5.698825e-01 -5.803963e-01  -6.870806e-01   \n",
       "50%     -2.458409e-01 -2.292746e-01 -2.370459e-01  -1.756999e-01   \n",
       "75%      2.604547e-01  2.684162e-01  2.793106e-01   4.561338e-01   \n",
       "max      1.381603e+01  3.071047e+01  1.293803e+01   5.839969e+00   \n",
       "\n",
       "                  a             b             c  \n",
       "count  1.651200e+04  1.651200e+04  1.651200e+04  \n",
       "mean   8.014691e-17 -1.621765e-17 -4.878742e-17  \n",
       "std    1.000030e+00  1.000030e+00  1.000030e+00  \n",
       "min   -1.650273e+00 -2.075303e-01 -2.704542e+00  \n",
       "25%   -3.822514e-01 -5.741738e-02 -5.914834e-01  \n",
       "50%   -7.966522e-02 -2.406537e-02 -1.628895e-01  \n",
       "75%    2.358755e-01  1.596812e-02  4.044268e-01  \n",
       "max    5.225419e+01  1.070603e+02  3.975916e+01  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_num_tr = pd.DataFrame(housing_num_tr, columns=list(housing_num.columns)+list(\"abc\"))\n",
    "housing_num_tr.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different data preparation techniques on different columns using ColumnTransformer\n",
    "Sometimes we want to perform different data preparation techniques on different columns. For example, you may want to impute missing values with a median value, then scale the values and impute missing categorical values using the most frequent value and one hot encode the categories.\n",
    "\n",
    "The ColumnTransformer allows to selectively apply data preparation transforms on different columns. You can apply a specific sequence of transforms to just the numerical columns, and a nother sequence of transforms to the categorical columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each transformer is a three-element tuple that defines: \n",
    "1. transformer name\n",
    "2. the transform to apply\n",
    "3. the column indices to apply it to.\n",
    "\n",
    "For example: (Name, Object, Columns)\n",
    "set remainder=’passthrough’  to keep unspecified columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>1.651200e+04</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-119.575834</td>\n",
       "      <td>35.639577</td>\n",
       "      <td>28.653101</td>\n",
       "      <td>-5.647937e-19</td>\n",
       "      <td>533.998123</td>\n",
       "      <td>1419.790819</td>\n",
       "      <td>497.060380</td>\n",
       "      <td>3.875589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.001860</td>\n",
       "      <td>2.138058</td>\n",
       "      <td>12.574726</td>\n",
       "      <td>1.000030e+00</td>\n",
       "      <td>410.839621</td>\n",
       "      <td>1115.686241</td>\n",
       "      <td>375.720845</td>\n",
       "      <td>1.904950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-124.350000</td>\n",
       "      <td>32.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.223689e+00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.499900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-121.800000</td>\n",
       "      <td>33.940000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>-5.516890e-01</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>784.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>2.566775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-118.510000</td>\n",
       "      <td>34.260000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>-2.353301e-01</td>\n",
       "      <td>433.000000</td>\n",
       "      <td>1164.000000</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>3.540900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-118.010000</td>\n",
       "      <td>37.720000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>2.423650e-01</td>\n",
       "      <td>641.000000</td>\n",
       "      <td>1719.250000</td>\n",
       "      <td>602.000000</td>\n",
       "      <td>4.744475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-114.310000</td>\n",
       "      <td>41.950000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>1.716114e+01</td>\n",
       "      <td>6210.000000</td>\n",
       "      <td>35682.000000</td>\n",
       "      <td>5358.000000</td>\n",
       "      <td>15.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          longitude      latitude  housing_median_age   total_rooms  \\\n",
       "count  16512.000000  16512.000000        16512.000000  1.651200e+04   \n",
       "mean    -119.575834     35.639577           28.653101 -5.647937e-19   \n",
       "std        2.001860      2.138058           12.574726  1.000030e+00   \n",
       "min     -124.350000     32.540000            1.000000 -1.223689e+00   \n",
       "25%     -121.800000     33.940000           18.000000 -5.516890e-01   \n",
       "50%     -118.510000     34.260000           29.000000 -2.353301e-01   \n",
       "75%     -118.010000     37.720000           37.000000  2.423650e-01   \n",
       "max     -114.310000     41.950000           52.000000  1.716114e+01   \n",
       "\n",
       "       total_bedrooms    population    households  median_income  \n",
       "count    16512.000000  16512.000000  16512.000000   16512.000000  \n",
       "mean       533.998123   1419.790819    497.060380       3.875589  \n",
       "std        410.839621   1115.686241    375.720845       1.904950  \n",
       "min          2.000000      3.000000      2.000000       0.499900  \n",
       "25%        296.000000    784.000000    279.000000       2.566775  \n",
       "50%        433.000000   1164.000000    408.000000       3.540900  \n",
       "75%        641.000000   1719.250000    602.000000       4.744475  \n",
       "max       6210.000000  35682.000000   5358.000000      15.000100  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16512, 9)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply simple imputer to total_rooms and pupulation columns \n",
    "from sklearn.compose import ColumnTransformer\n",
    "t = [('roomspop', SimpleImputer(strategy='median'), [3, 5])]\n",
    "transformer = ColumnTransformer(transformers=t, remainder='passthrough' )\n",
    "tt = transformer.fit_transform(housing)\n",
    "tt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16512, 16)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "full_pipeline = ColumnTransformer([(\"num\", num_pipeline, num_attribs), (\"cat\", OneHotEncoder(), cat_attribs),])\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "housing_prepared.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating on the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.30776372e+04 -5.39096285e+04  1.37166194e+04 -9.61749828e+03\n",
      "  2.96598024e+04 -4.44381176e+04  2.95164511e+04  7.36319754e+04\n",
      " -6.79249357e+02  8.63962387e+02 -8.88446355e+02 -7.68781994e+16\n",
      " -7.68781994e+16 -7.68781994e+16 -7.68781994e+16 -7.68781994e+16]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "reg = lin_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.15604281,  0.77194962,  0.74333089, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.17602483,  0.6596948 , -1.1653172 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.18684903, -1.34218285,  0.18664186, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       ...,\n",
       "       [ 1.58648943, -0.72478134, -1.56295222, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.78221312, -0.85106801,  0.18664186, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.43579109,  0.99645926,  1.85670895, ...,  0.        ,\n",
       "         1.        ,  0.        ]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [211760. 321552. 210864.  61600. 192336.]\n"
     ]
    }
   ],
   "source": [
    "# let's try the full preprocessing pipeline on a few training instances\n",
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "\n",
    "print(\"Predictions:\", lin_reg.predict(some_data_prepared))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare againest actual values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels:\", list(some_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.15604281,  0.77194962,  0.74333089, -0.49323393, -0.44543821,\n",
       "        -0.63621141, -0.42069842, -0.61493744,  0.05964071, -0.08649871,\n",
       "        -0.01080657,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [-1.17602483,  0.6596948 , -1.1653172 , -0.90896655, -1.0369278 ,\n",
       "        -0.99833135, -1.02222705,  1.33645936, -0.44706653, -0.03353391,\n",
       "        -0.00223757,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 1.18684903, -1.34218285,  0.18664186, -0.31365989, -0.15334458,\n",
       "        -0.43363936, -0.0933178 , -0.5320456 ,  0.11932063, -0.09240499,\n",
       "        -0.02222581,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         1.        ],\n",
       "       [-0.01706767,  0.31357576, -0.29052016, -0.36276217, -0.39675594,\n",
       "         0.03604096, -0.38343559, -1.04556555,  0.09250197,  0.08973561,\n",
       "        -0.01530315,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.49247384, -0.65929936, -0.92673619,  1.85619316,  2.41221109,\n",
       "         2.72415407,  2.57097492, -0.44143679,  0.26910113, -0.00419445,\n",
       "         0.01135559,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_data_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69035.8591177743"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49894.03167393411"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "lin_mae = mean_absolute_error(housing_labels, housing_predictions)\n",
    "lin_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(random_state=42)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better Evaluation Using Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([69347.70144216, 69853.09606991, 70085.01188048, 69488.44267631,\n",
       "       72049.19849013, 74715.70639183, 71650.88681675, 69074.07635218,\n",
       "       74325.648709  , 71999.08930151])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "tree_rmse_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [69347.70144216 69853.09606991 70085.01188048 69488.44267631\n",
      " 72049.19849013 74715.70639183 71650.88681675 69074.07635218\n",
      " 74325.648709   71999.08930151]\n",
      "Mean: 71258.88581302542\n",
      "Standard deviation: 1938.9562935871093\n"
     ]
    }
   ],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [67442.07735663 67471.71986433 68389.37607147 74858.10236572\n",
      " 68267.60509178 71591.67036063 65397.8798693  68667.99360518\n",
      " 73018.62932732 68057.75776951]\n",
      "Mean: 69316.28116818739\n",
      "Standard deviation: 2753.697652337107\n"
     ]
    }
   ],
   "source": [
    "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(random_state=42)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "forest_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18889.426957929023"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_predictions = forest_reg.predict(housing_prepared)\n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [50342.32597027 48158.37919093 50431.76893217 53059.2744423\n",
      " 50824.93619062 54470.64683468 49401.68899025 48587.02754089\n",
      " 54059.32301416 51396.65758741]\n",
      "Mean: 51073.2028693681\n",
      "Standard deviation: 2073.3335629742364\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
    "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111768.39440197597"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_reg = SVR(kernel=\"linear\")\n",
    "svm_reg.fit(housing_prepared, housing_labels)\n",
    "housing_predictions = svm_reg.predict(housing_prepared)\n",
    "svm_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "svm_rmse = np.sqrt(svm_mse)\n",
    "svm_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tune Your Model\n",
    "* manual tuning \n",
    "* Use GridSearchCV\n",
    "* 3 * 4 + 2 * 3 = 18 different combination of parameters \n",
    "* a total of 18 * 5 (CV) = 90 training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=42),\n",
       "             param_grid=[{'max_features': [2, 4, 6, 8],\n",
       "                          'n_estimators': [3, 10, 30]},\n",
       "                         {'bootstrap': [False], 'max_features': [2, 3, 4],\n",
       "                          'n_estimators': [3, 10]}],\n",
       "             return_train_score=True, scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    # try 12 (3×4) combinations of hyperparameters\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    # then try 6 (2×3) combinations with bootstrap set as False\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_features=8, n_estimators=30, random_state=42)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 8, 'n_estimators': 30}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67578.6539718191 {'max_features': 2, 'n_estimators': 3}\n",
      "57483.586136217105 {'max_features': 2, 'n_estimators': 10}\n",
      "54531.493402760345 {'max_features': 2, 'n_estimators': 30}\n",
      "60977.17003204092 {'max_features': 4, 'n_estimators': 3}\n",
      "53417.32754699844 {'max_features': 4, 'n_estimators': 10}\n",
      "50879.37230398769 {'max_features': 4, 'n_estimators': 30}\n",
      "58292.68942265689 {'max_features': 6, 'n_estimators': 3}\n",
      "52376.156974235695 {'max_features': 6, 'n_estimators': 10}\n",
      "50265.881429303976 {'max_features': 6, 'n_estimators': 30}\n",
      "59158.62976047877 {'max_features': 8, 'n_estimators': 3}\n",
      "52658.00187724696 {'max_features': 8, 'n_estimators': 10}\n",
      "50169.10645977438 {'max_features': 8, 'n_estimators': 30}\n",
      "65142.84908114044 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n",
      "56295.55453646887 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n",
      "60058.60136087842 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n",
      "53781.14851277873 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n",
      "59835.22821934388 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n",
      "52773.286096488 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(random_state=42),\n",
       "                   param_distributions={'max_features': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000002A205452A60>,\n",
       "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000002A205477EE0>},\n",
       "                   random_state=42, scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distribs = {\n",
    "        'n_estimators': randint(low=1, high=200),\n",
    "        'max_features': randint(low=1, high=8),\n",
    "    }\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n",
    "                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
    "rnd_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49252.818341365186 {'max_features': 7, 'n_estimators': 180}\n",
      "51715.6236357881 {'max_features': 5, 'n_estimators': 15}\n",
      "51545.03346553677 {'max_features': 3, 'n_estimators': 72}\n",
      "50971.19618041091 {'max_features': 5, 'n_estimators': 21}\n",
      "49404.21625325934 {'max_features': 7, 'n_estimators': 122}\n",
      "51513.16815578037 {'max_features': 3, 'n_estimators': 75}\n",
      "51408.48915527614 {'max_features': 3, 'n_estimators': 88}\n",
      "49848.692582994976 {'max_features': 5, 'n_estimators': 100}\n",
      "51276.40554204698 {'max_features': 3, 'n_estimators': 150}\n",
      "64913.737262479306 {'max_features': 5, 'n_estimators': 2}\n"
     ]
    }
   ],
   "source": [
    "cvres = rnd_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07755933, 0.07068123, 0.04592016, 0.01630995, 0.01579577,\n",
       "       0.0166108 , 0.01548961, 0.41719945, 0.01654275, 0.11381314,\n",
       "       0.01670432, 0.01307243, 0.15953212, 0.00004678, 0.00165716,\n",
       "       0.003065  ])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Your System on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the Best Models and Their Errorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07755933, 0.07068123, 0.04592016, 0.01630995, 0.01579577,\n",
       "       0.0166108 , 0.01548961, 0.41719945, 0.01654275, 0.11381314,\n",
       "       0.01670432, 0.01307243, 0.15953212, 0.00004678, 0.00165716,\n",
       "       0.003065  ])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.4171994545836687, 'median_income'),\n",
       " (0.1595321228648014, 'INLAND'),\n",
       " (0.11381314300339597, 'pop_per_hhold'),\n",
       " (0.07755932810965623, 'longitude'),\n",
       " (0.07068122855538019, 'latitude'),\n",
       " (0.04592016433402573, 'housing_median_age'),\n",
       " (0.016704317132672387, 'bedrooms_per_room'),\n",
       " (0.01661079654930104, 'population'),\n",
       " (0.01654275380085179, 'rooms_per_hhold'),\n",
       " (0.016309948421199275, 'total_rooms'),\n",
       " (0.015795765212541498, 'total_bedrooms'),\n",
       " (0.015489607854418391, 'households'),\n",
       " (0.01307243345462553, '<1H OCEAN'),\n",
       " (0.0030649953554062427, 'NEAR OCEAN'),\n",
       " (0.0016571599815796592, 'NEAR BAY'),\n",
       " (4.678078647600746e-05, 'ISLAND')]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "#cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"] # old solution\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
